name: Daily PDF Batch Processing

on:
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:  # Allow manual triggering

jobs:
  daily-batch-process:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
        
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y openjdk-11-jdk
        sudo apt-get install -y ghostscript
        sudo apt-get install -y tesseract-ocr
        
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Create directories
      run: |
        mkdir -p input_pdfs
        mkdir -p batch_output
        mkdir -p processed_pdfs
        mkdir -p daily_reports
        
    - name: Run daily batch processing
      run: |
        python batch_pdf_processor.py input_pdfs batch_output --workers 4
      env:
        PYTHONPATH: ${{ github.workspace }}
        
    - name: Generate daily summary
      run: |
        python -c "
        import json
        import os
        from datetime import datetime, timedelta
        
        # Read batch summary if it exists
        summary_file = 'batch_output/batch_summary.json'
        if os.path.exists(summary_file):
            with open(summary_file, 'r') as f:
                summary = json.load(f)
            
            # Create comprehensive daily report
            report = f'''# Daily Processing Report - {datetime.now().strftime('%Y-%m-%d')}
            
            ## Executive Summary
            - **Date**: {datetime.now().strftime('%Y-%m-%d')}
            - **Total Files Processed**: {summary.get('total_files', 0)}
            - **Success Rate**: {(summary.get('successful_files', 0) / max(summary.get('total_files', 1), 1) * 100):.1f}%
            - **Total Tables Extracted**: {summary.get('total_tables_found', 0)}
            - **Total Processing Time**: {summary.get('total_processing_time', 0):.2f} seconds
            - **Average Time per File**: {(summary.get('total_processing_time', 0) / max(summary.get('total_files', 1), 1)):.2f} seconds
            
            ## Performance Metrics
            - **Files Successfully Processed**: {summary.get('successful_files', 0)}
            - **Files Failed**: {summary.get('failed_files', 0)}
            - **Processing Efficiency**: {summary.get('total_tables_found', 0) / max(summary.get('total_processing_time', 1), 1):.2f} tables/second
            
            ## Detailed Results
            '''
            
            successful_files = []
            failed_files = []
            
            for result in summary.get('file_results', []):
                if result.get('status') == 'success':
                    successful_files.append(result)
                    report += f'### ✅ {result.get('filename')}\n'
                    report += f'- **Tables Found**: {result.get('total_tables', 0)}\n'
                    report += f'- **Processing Time**: {result.get('processing_time_seconds', 0):.2f}s\n'
                    report += f'- **Text Length**: {result.get('text_length', 0):,} characters\n'
                    report += f'- **Word Count**: {result.get('word_count', 0):,} words\n\n'
                else:
                    failed_files.append(result)
                    report += f'### ❌ {result.get('filename')}\n'
                    report += f'- **Error**: {result.get('error', 'Unknown error')}\n\n'
            
            # Add statistics
            if successful_files:
                avg_tables = sum(f.get('total_tables', 0) for f in successful_files) / len(successful_files)
                avg_time = sum(f.get('processing_time_seconds', 0) for f in successful_files) / len(successful_files)
                report += f'## Statistics\n'
                report += f'- **Average Tables per File**: {avg_tables:.1f}\n'
                report += f'- **Average Processing Time**: {avg_time:.2f} seconds\n'
                report += f'- **Total Text Extracted**: {sum(f.get('text_length', 0) for f in successful_files):,} characters\n'
            
            # Save daily report
            daily_report_file = f'daily_reports/daily_report_{datetime.now().strftime('%Y-%m-%d')}.md'
            with open(daily_report_file, 'w') as f:
                f.write(report)
                
            # Also save as JSON for programmatic access
            daily_summary = {
                'date': datetime.now().strftime('%Y-%m-%d'),
                'total_files': summary.get('total_files', 0),
                'successful_files': summary.get('successful_files', 0),
                'failed_files': summary.get('failed_files', 0),
                'total_tables': summary.get('total_tables_found', 0),
                'total_processing_time': summary.get('total_processing_time', 0),
                'success_rate': (summary.get('successful_files', 0) / max(summary.get('total_files', 1), 1) * 100),
                'files_processed': [f.get('filename') for f in summary.get('file_results', []) if f.get('status') == 'success']
            }
            
            with open(f'daily_reports/daily_summary_{datetime.now().strftime('%Y-%m-%d')}.json', 'w') as f:
                json.dump(daily_summary, f, indent=2)
        else:
            with open(f'daily_reports/daily_report_{datetime.now().strftime('%Y-%m-%d')}.md', 'w') as f:
                f.write('# No PDFs Processed Today\nNo PDF files found in input_pdfs directory.')
        "
        
    - name: Archive processed PDFs
      run: |
        if [ -d "input_pdfs" ] && [ "$(ls -A input_pdfs)" ]; then
          # Create archive directory with date
          archive_dir="processed_pdfs/$(date '+%Y-%m-%d')"
          mkdir -p "$archive_dir"
          mv input_pdfs/* "$archive_dir/" 2>/dev/null || true
        fi
        
    - name: Commit daily results
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        # Add all changes
        git add batch_output/
        git add daily_reports/
        git add processed_pdfs/
        
        # Commit with date
        git commit -m "Daily batch processing - $(date '+%Y-%m-%d')" || echo "No changes to commit"
        
        # Push to repository
        git push origin main || echo "No changes to push"
        
    - name: Upload daily results
      uses: actions/upload-artifact@v4
      with:
        name: daily-processing-results-$(date '+%Y-%m-%d')
        path: |
          batch_output/
          daily_reports/
          processed_pdfs/
        retention-days: 90 